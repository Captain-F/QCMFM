{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64f35a-ce36-4c30-a93c-37c0019eda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "#from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "import time\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd4c77-10d1-4081-9b21-916ce11ef10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    num_heads = 5\n",
    "    num_layers = 7 \n",
    "    dim_crossAtt = 500 \n",
    "    dim_amp = pow(2, 6) \n",
    "    n_qubits = 4\n",
    "    bsz = 32\n",
    "    epochs = 50\n",
    "    inter_step = int(n_qubits/2) + 2\n",
    "    \n",
    "\n",
    "def read_pickle(path):\n",
    "    with open(path, 'rb')as f:\n",
    "        feats = pickle.load(f)\n",
    "    return feats\n",
    "\n",
    "def save_pickle(path, feats):\n",
    "    with open(path, 'wb')as f:\n",
    "        pickle.dump(feats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f49b8d-d32f-412f-a6a3-64c8137df78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=Args.n_qubits)\n",
    "@qml.qnode(dev)\n",
    "def circuit(inputs, **weights):\n",
    "    \n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(Args.n_qubits), normalize=True)\n",
    "\n",
    "    for layer in range(Args.num_layers): \n",
    "\n",
    "        # 编码；\n",
    "        s = Args.n_qubits * 6 * layer\n",
    "        for idx, i in enumerate(range(Args.n_qubits)):\n",
    "            qml.RX(weights[\"weights_\"+str(i+s)], wires=idx)\n",
    "    \n",
    "        for idx, i in enumerate(range(Args.n_qubits)):\n",
    "            qml.RZ(weights[\"weights_\"+str(i+Args.n_qubits+s)], wires=idx)\n",
    "    \n",
    "        for idx, i in enumerate(range(Args.n_qubits)):\n",
    "            qml.RY(weights[\"weights_\"+str(i+Args.n_qubits*2+s)], wires=idx) \n",
    "\n",
    "    \n",
    "    # 进行intra-modal fusion; 量子纠缠 (无参数)；\n",
    "    n_qubits_img = [i for i in range(int(Args.n_qubits/2))]\n",
    "    n_qubits_txt = [i+int(Args.n_qubits/2) for i in range(int(Args.n_qubits/2))]\n",
    "\n",
    "    # 图片 intra_modal；\n",
    "    for i in n_qubits_img:\n",
    "        if i < n_qubits_img[-1]:\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "        else:\n",
    "            qml.CNOT(wires=[i, 0])\n",
    "\n",
    "    # 文本 intra_modal；\n",
    "    for i in n_qubits_txt:\n",
    "        if i < n_qubits_txt[-1]:\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "        else:\n",
    "            qml.CNOT(wires=[i, n_qubits_txt[0]])\n",
    "\n",
    "    # 进行角度编码 <X, Z, Y>；\n",
    "    for layer in range(Args.num_layers): \n",
    "        s = Args.n_qubits * 6 * layer\n",
    "\n",
    "        # 进行量子编码；\n",
    "        for idx, i in enumerate(range(Args.n_qubits)):\n",
    "            qml.RX(weights[\"weights_\"+str(i+Args.n_qubits*3+s)], wires=idx)\n",
    "    \n",
    "        for idx, i in enumerate(range(Args.n_qubits)):\n",
    "            qml.RZ(weights[\"weights_\"+str(i+Args.n_qubits*4+s)], wires=idx)\n",
    "    \n",
    "        for idx, i in enumerate(range(Args.n_qubits)):\n",
    "            qml.RY(weights[\"weights_\"+str(i+Args.n_qubits*5+s)], wires=idx)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "    if int(Args.n_qubits/2) == Args.inter_step:\n",
    "    \n",
    "        # 多模态融合 inter-modal；\n",
    "        for i in range(Args.n_qubits):\n",
    "            if i < int(Args.n_qubits/2):\n",
    "                qml.CNOT(wires=[i, i+int(Args.n_qubits/2)])\n",
    "            else:\n",
    "                qml.CNOT(wires=[i, i-int(Args.n_qubits/2)])\n",
    "\n",
    "    else:\n",
    "        # 多模态融合 inter-modal；\n",
    "        loc_0, loc_1 = [], []\n",
    "        for i in range(Args.n_qubits):\n",
    "            if i < int(Args.n_qubits/2):\n",
    "                if i + Args.inter_step <= Args.n_qubits - 1:\n",
    "                    loc = [i, i+Args.inter_step]\n",
    "                    loc_0.append(loc)\n",
    "                else:\n",
    "                    loc = [i, i+Args.inter_step-int(Args.n_qubits/2)]\n",
    "                    loc_0.append(loc)\n",
    "            else:\n",
    "                dic_reverse = dict([[i[1], i[0]] for i in loc_0])\n",
    "                loc = [i, dic_reverse[i]]\n",
    "                loc_1.append(loc)\n",
    "        loc = loc_0 + loc_1\n",
    "        for l in loc:\n",
    "            qml.CNOT(wires=[l[0], l[1]])            \n",
    "#####################################################\n",
    "    \n",
    "    \n",
    "    # 进行量子测量；\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(Args.n_qubits)]\n",
    "\n",
    "\n",
    "weight_shapes = {\"weights_\"+str(i): 1 for i in range(Args.n_qubits*6*Args.num_layers)}\n",
    "\n",
    "init_method_ = torch.nn.init.normal_\n",
    "qlayer = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes, init_method=init_method_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4261a4-2d56-42be-be66-b31df9aebb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCMFM(nn.Module):\n",
    "  # Quanvolution --> Convolution;\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=2048, out_channels=512,\n",
    "              kernel_size=2, stride=1, padding=0),\n",
    "        nn.ReLU(), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "    # 放缩到同一个维度；\n",
    "    self.fc_txt = nn.Sequential(nn.Linear(768, Args.dim_crossAtt), nn.ReLU())\n",
    "    self.fc_img = nn.Sequential(nn.Linear(512, Args.dim_crossAtt), nn.ReLU())\n",
    "    \n",
    "    # 正则化；\n",
    "    self.lm_t = nn.LayerNorm(Args.dim_crossAtt)\n",
    "    self.lm_i = nn.LayerNorm(Args.dim_crossAtt)\n",
    "\n",
    "    # 跨模态交叉注意力机制；\n",
    "    self.att_img_txt = nn.MultiheadAttention(embed_dim=Args.dim_crossAtt, num_heads=Args.num_heads, batch_first=True)\n",
    "    self.att_txt_img = nn.MultiheadAttention(embed_dim=Args.dim_crossAtt, num_heads=Args.num_heads, batch_first=True)\n",
    "\n",
    "    # 线性层：图片和文本；\n",
    "    self.fc_t_res = nn.Sequential(nn.Linear(Args.dim_crossAtt*30, int(pow(2, Args.n_qubits)/2)), nn.ReLU())\n",
    "    self.fc_i_res = nn.Sequential(nn.Linear(Args.dim_crossAtt*49, int(pow(2, Args.n_qubits)/2)), nn.ReLU())\n",
    "    \n",
    "    # 量子线路多模态融合；\n",
    "    self.qlayer = qlayer\n",
    "\n",
    "    # 量子线路融合特征非线性激活；\n",
    "    self.fc_q = nn.Linear(Args.n_qubits, 4)\n",
    "    self.name = \"HQCMFM-qubits=\" + str(Args.n_qubits) + \"-layers=\" + str(Args.num_layers) + \"-step=\" + str(Args.inter_step)\n",
    "\n",
    "  def forward(self, x_img, x_txt,):\n",
    "    x_img = x_img.view(-1, 49, 512) # 图片\n",
    "    x_txt = x_txt.view(-1, 30, 768) # 文本\n",
    "\n",
    "    # 放缩到同一个维度；\n",
    "    x_img = self.fc_img(x_img)\n",
    "    x_txt = self.fc_txt(x_txt)\n",
    "\n",
    "    # 正则化；\n",
    "    x_img = self.lm_i(x_img)\n",
    "    x_txt = self.lm_t(x_txt)\n",
    "\n",
    "    # 跨模态交叉注意力机制；\n",
    "    x_img_att, i_att_weights = self.att_img_txt(query=x_img, key=x_txt, value=x_txt)\n",
    "    x_txt_att, t_att_weights = self.att_txt_img(query=x_txt, key=x_img, value=x_img)\n",
    "\n",
    "    # reshape;\n",
    "    x_img_att_res = x_img_att.reshape(x_img_att.shape[0], 500*49)\n",
    "    x_txt_att_res = x_txt_att.reshape(x_txt_att.shape[0], 500*30)\n",
    "\n",
    "    # 线性层；拼接\n",
    "    x_img_att_res = self.fc_i_res(x_img_att_res)\n",
    "    x_txt_att_res = self.fc_t_res(x_txt_att_res)\n",
    "    x_conca = torch.cat([x_img_att_res, x_txt_att_res], dim=-1)\n",
    "\n",
    "    # 量子多模态融合；\n",
    "    x_qfusion = self.qlayer(x_conca)\n",
    "\n",
    "    # 融合后的输出；\n",
    "    x_qfusion = self.fc_q(x_qfusion)\n",
    "\n",
    "    return F.log_softmax(x_qfusion, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db520395-7b52-48e9-9df2-e5774a61d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "textiles_data_seq = read_pickle(\"data.pickle\")\n",
    "\n",
    "imgs = np.concatenate([textiles_data_seq['chou']['img'], textiles_data_seq['duan']['img'], \n",
    "                       textiles_data_seq['jin']['img'], textiles_data_seq['si']['img']], \n",
    "                       axis=0)\n",
    "\n",
    "txts = np.concatenate([textiles_data_seq['chou']['txt_emb'], textiles_data_seq['duan']['txt_emb'], \n",
    "                       textiles_data_seq['jin']['txt_emb'], textiles_data_seq['si']['txt_emb']], \n",
    "                       axis=0)\n",
    "\n",
    "labs = textiles_data_seq['chou']['labs'] + textiles_data_seq['duan']['labs'] + \\\n",
    "       textiles_data_seq['jin']['labs'] + textiles_data_seq['si']['labs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57104131-302a-4b9b-a77b-bec79e175109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取图片特征；\n",
    "\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Add, TimeDistributed, Multiply, Softmax, Reshape, MultiHeadAttention\n",
    "from tensorflow.python.keras.utils import np_utils \n",
    "\n",
    "base_model = VGG19(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "head_model = base_model.output\n",
    "pre_model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "imgs = pre_model.predict(imgs, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016a955-4e7a-4ad1-bd53-534b43ff17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集；\n",
    "\n",
    "class LpDataset(Dataset):\n",
    "\n",
    "  def __init__(self, imgs, txts, labels):\n",
    "    \"\"\"\n",
    "    :param labels: [0, 1, 2, ...]\n",
    "    \"\"\"\n",
    "    self.name2label = {\"falang\": 0, \"jinyin\": 1, \"qiqi\": 2, \n",
    "                       \"tongqi\": 3, \"yushi\": 4}\n",
    "    self.labels = labels\n",
    "    self.imgs = imgs\n",
    "    self.txts = txts\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img, txt, label = self.imgs[idx], self.txts[idx], self.labels[idx]\n",
    "\n",
    "    return img, txt, label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "\n",
    "trainData = LpDataset(imgs=trainX_img, txts=trainX_txt, labels=trainY)\n",
    "testData = LpDataset(imgs=testX_img, txts=testX_txt, labels=testY)\n",
    "\n",
    "trainLoader = DataLoader(dataset=trainData, batch_size=Args.bsz,\n",
    "              sampler=torch.utils.data.RandomSampler(trainData),\n",
    "              num_workers=0,\n",
    "              pin_memory=True)\n",
    "testLoader = DataLoader(dataset=testData, batch_size=Args.bsz,\n",
    "              sampler=torch.utils.data.RandomSampler(testData),\n",
    "              num_workers=0,\n",
    "              pin_memory=True)\n",
    "\n",
    "\n",
    "def train(trainLoader, model, device, optimizer):\n",
    "    for idx, trainD in enumerate(trainLoader):\n",
    "        #print(trainD[1].shape)\n",
    "        inputs_i, inputs_t, targets = trainD[0], trainD[1], trainD[2]\n",
    "        inputs_i, inputs_t, targets = inputs_i.to(device), inputs_t.to(device), targets.to(device)\n",
    "        outputs = model(inputs_i, inputs_t)\n",
    "        loss = F.nll_loss(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #logger.info(\"loss: %f\"%loss.item())\n",
    "        print(f\"loss: {loss.item()}\", end=\"\\r\")\n",
    "\n",
    "def test(testLoader, model, device):\n",
    "    target_all = []\n",
    "    output_all = []\n",
    "    with torch.no_grad():\n",
    "        for idx, testD in enumerate(testLoader):\n",
    "            inputs_i, inputs_t, targets = testD[0], testD[1], testD[2]\n",
    "            inputs_i, inputs_t, targets = inputs_i.to(device), inputs_t.to(device), targets.to(device)\n",
    "            outputs = model(inputs_i, inputs_t)\n",
    "\n",
    "            target_all.append(targets)\n",
    "            output_all.append(outputs)\n",
    "        target_all = torch.cat(target_all, dim=0)\n",
    "        output_all = torch.cat(output_all, dim=0)\n",
    "\n",
    "    _, indices = output_all.topk(1, dim=1)\n",
    "    masks = indices.eq(target_all.view(-1, 1).expand_as(indices))\n",
    "    size = target_all.shape[0]\n",
    "    corrects = masks.sum().item()\n",
    "    accuracy = corrects / size\n",
    "    loss = F.nll_loss(output_all, target_all).item()\n",
    "\n",
    "    print(f\"test set accuracy: {accuracy}\")\n",
    "    print(f\"test set loss: {loss}\")\n",
    "\n",
    "    return accuracy, loss, target_all, output_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3edb3-7d40-47a5-a168-6cac115f42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# 进行模型加载；\n",
    "model = QCMFM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=Args.epochs)\n",
    "accs, y_preds, y_trues = [], [], []\n",
    "\n",
    "f1_best = 0.\n",
    "for epoch in range(1, Args.epochs+1):\n",
    "    # train\n",
    "    print(f\"******Train {epoch} epoch*******\")\n",
    "    s_time = time.time()\n",
    "    train(trainLoader, model, device, optimizer)\n",
    "    e_time = time.time()\n",
    "    print(f\"|| Training time: {e_time-s_time}\")\n",
    "    #times.append((e_time-s_time))\n",
    "    \n",
    "    # test\n",
    "    acc_tes, loss_test, y_true_test, y_pred_test = test(testLoader, model, device)\n",
    "    try:\n",
    "      y_true_test_, y_pred_test_ = y_true_test.cpu(), y_pred_test.cpu().argmax(axis=-1)\n",
    "    except:\n",
    "      y_true_test_, y_pred_test_ = y_true_test, y_pred_test.argmax(axis=-1)\n",
    "\n",
    "    # 保存最优模型，以F1值为标准；\n",
    "    f1 = f1_score(y_true_test_, y_pred_test_, average=\"macro\")\n",
    "    if f1 > f1_best:\n",
    "        f1_best = f1\n",
    "        torch.save(model.state_dict(), \"../pt/textiles/\"+model.name+\".pt\")\n",
    "        save_pickle(\"../pt/textiles/\"+model.name+\".pickle\", y_pred_test)\n",
    "        save_pickle(\"../pt/textiles/\"+model.name+\"-yTrueTest.pickle\", y_true_test_)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    print(classification_report(y_true_test_, y_pred_test_, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quanmhum",
   "language": "python",
   "name": "quanmhum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
